%% Согласно ГОСТ Р 7.0.11-2011:
%% 5.3.3 В заключении диссертации излагают итоги выполненного исследования, рекомендации, перспективы дальнейшей разработки темы.
%% 9.2.3 В заключении автореферата диссертации излагают итоги данного исследования, рекомендации и перспективы дальнейшей разработки темы.

\begin{enumerate}[label={\arabic*)}]
	\item Разработан автоматический алгоритм разбиения слов на однородные части, в основе которого нахождение положения границ частей производится с помощью многопараметрической оптимизации.
	Сформулированы критерии, реализующие принцип максимизации меры сходства фонетического материала внутри части и меры различия между соседними частями. 
	Для численного решения задачи с высоким быстродействием предложены алгоритмы, основанные на методе динамического программирования.
	Эксперименты, проведённые на примерах нескольких слов русского языка, подтвердили работоспособность предложенного подхода и правомерность принятых допущений.
	\item Разработан алгоритм улучшения качества эталона, основанный на выделении и оптимизации главных компонент.
	Эталон, полученный с помощью оптимизации коэффициентов при главных компонентах, показал значительно меньшее число ошибок при распознавании большинства записей.
	Общее количество ошибок для записей слов с шумом в наушниках до оптимизации равнялось 5~\%, а после оптимизации уменьшилось до 1.25~\%.
	Также был сделан вывод о том, что для получения приемлемых результатов достаточно использовать только одну реализацию слова и проводить всего 10 итераций при получении оптимального эталона, что заметно сокращает время работы программы.
	\item Изучены способы и разработаны алгоритмы сжатия информации о параметрическом портрете с помощью применения полиномов Чебышёва.
	Эксперименты показали, что сжатие может происходить как отдельно по частотам и по времени, так и по обоим измерениям одновременно.
	В последнем случае можно сократить место для хранения параметрического портрета в 5--10 раз практически без ухудшения качества распознавания.	
	\item Разработаны алгоритмы на основе формулы Байеса и метода комитетов, позволяющие заметно уменьшить количество ошибок распознавания при использовании нескольких эталонов.
	Первый алгоритм использует оценки априорных вероятностей, определяемые по обучающей выборке, и рассчитывает апостериорные вероятности формулы Байеса, а второй является модификацией известного метода комитетов.
	Выявленная в ходе тестирования возможность локализации распознаваемого слова с точностью до малой группы позволяет повышать быстродействие систем распознавания на основе иерархических процедур, в которых последовательно применяются алгоритмы распознавания разных видов.
	Работоспособность обоих разработанных алгоритмов подтверждается результатами тестирования.
	При использовании 7 эталонов, полученных по записям различных дикторов, достигается заметное снижение процента ошибок в 1.5--2 раза --- средняя ошибка для алгоритма на основе формулы Байеса снизилась с 8.42 до 5.62~\%, а для алгоритма на основе метода комитетов до 5.3 и до 3.13~\% при использовании подстройки по времени.
	\item Изучены и модифицированы алгоритмы распознавания речевых команд на основе искусственных нейронных сетей глубокого обучения.
	Наилучшие результаты показала архитектура нейронных сетей CNN с двумя слоями свёртки и тремя полностью связанными слоями.
	При обучении на словаре из 20 слов без шума на 7 дикторах средняя величина ошибки при распознавании <<чужих>> дикторов равна 0.6~\%.
	При обучении в той же конфигурации, но на записях с добавленным шумом, величина ошибки достигает для <<чужого>> диктора 1.1~\%.
	Эксперименты по распознаванию фраз при использовании обучающей выборки из записей 6 дикторов показали 4.2~\% ошибок для случая без шума и 7.0~\% для записей с шумом.
	Получены положительные результаты в дикторозависимом варианте распознавания без шума и в условиях шума при использовании небольшого числа записей каждой команды в обучающей выборке.
	Также получено значительное улучшение качества распознавания при добавлении всего нескольких реализаций каждой из речевых команд <<своего>> диктора в обучающую выборку, состоящую из записей <<чужих>> дикторов.
	При использовании нейронных сетей CNN количество ошибок является заметно более низким, чем для других алгоритмов, единственный замеченный недостаток --- это длительное время обучения нейронной сети.
\end{enumerate}
